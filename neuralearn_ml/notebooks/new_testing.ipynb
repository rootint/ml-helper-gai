{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/7myxq8l12wz8s_qhqdqq6d4r0000gn/T/ipykernel_56553/3308735501.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv('lora.tsv', sep='\\\\t')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question\\tAnswer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the key challenge with full fine-tunin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Low-Rank Adaptation (LoRA)?\\tLoRA is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does LoRA compare to full fine-tuning in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the impact of LoRA on inference latenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can LoRA be combined with other adaptation met...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Question\\tAnswer\n",
       "0  What is the key challenge with full fine-tunin...\n",
       "1  What is Low-Rank Adaptation (LoRA)?\\tLoRA is a...\n",
       "2  How does LoRA compare to full fine-tuning in t...\n",
       "3  What is the impact of LoRA on inference latenc...\n",
       "4  Can LoRA be combined with other adaptation met..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('lora.tsv', sep='\\t')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.gpt4all import GPT4AllEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    "    # separators='.'\n",
    ")\n",
    "\n",
    "# loader = TextLoader(\"mlops.txt\")\n",
    "loader = PyPDFLoader(\"lora.pdf\")\n",
    "# pages = loader.load_and_split()\n",
    "documents = loader.load()\n",
    "# nltk_splitter = NLTKTextSplitter()\n",
    "texts = text_splitter.split_documents(documents)\n",
    "# texts = nltk_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedder = GPT4AllEmbeddings()\n",
    "# db = FAISS.from_documents(texts, embedding=embedder)\n",
    "db = FAISS.load_local(\"ml_papers_2\", embedder)\n",
    "# db = FAISS.from_embeddings\n",
    "# db.save_local('db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is lora\"\n",
    "docs = db.similarity_search(query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/s per V100 GPU for LoRA.\n",
      "Model & Method\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_info(query):\n",
    "    similar_response = db.similarity_search(query, k=3)\n",
    "\n",
    "    page_contents_array = [doc.page_content for doc in similar_response]\n",
    "\n",
    "    # print(page_contents_array)\n",
    "\n",
    "    return page_contents_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "local_path = (\n",
    "    # \"/Users/random/.cache/gpt4all/ggml-model-gpt4all-falcon-q4_0.bin\"  # replace with your desired local file path\n",
    "    \"./gpt4all.gguf\"  # replace with your desired local file path\n",
    ")\n",
    "\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "llm = GPT4All(\n",
    "    model=local_path,\n",
    "    backend=\"llama\",\n",
    "    verbose=True,\n",
    "    streaming=True,\n",
    "    callbacks=callbacks,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "# template = \"\"\"You are a helpful AI assistant and provide the answer for the question based on the given context and your existing knowledge.\n",
    "# Context:{context}\n",
    "# >>QUESTION<<{message}\n",
    "# >>ANSWER<<\"\"\"\n",
    "\n",
    "template = \"\"\"\n",
    "[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Your answers are always brief. Base your answers on your knowledge and the context.\n",
    "<</SYS>>\n",
    "Context:{context}\n",
    "{message}[/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"message\"], template=template)\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(message):\n",
    "    context = retrieve_info(message)\n",
    "    # print(context)\n",
    "    response = llm_chain.run(message=message, context=context)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['//github.com/artidoro/qlora and https://github.com/TimDettmers', 'netuning. Furthermore, we want to analyze the components of QLoRA including the impact of\\nNormalFloat4 over standard Float4. The following sections will discuss the experiments that aimed\\nat answering these questions.\\nhttps:', '/65B\\nQLORA tuning on a single 24']\n",
      "\n",
      "QLoRA (Quantized Linear Regression Autoencoder) is a deep learning model designed for efficient and accurate text generation. It uses a combination of linear regression and autoencoders to generate high-quality text while minimizing the computational resources required. QLoRA can be used in various natural language processing tasks, such as text summarization or text generation."
     ]
    }
   ],
   "source": [
    "message = \"what is qlora\"\n",
    "context = retrieve_info(message)\n",
    "print(context)\n",
    "response = llm_chain.run(message=message, context=context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
