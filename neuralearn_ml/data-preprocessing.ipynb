{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search page names ans summary for ML related topics on Wikipedia using wikipedia library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while fetching summary for Machine learning: Page id \"machine ;earning\" does not match any pages. Try another id!\n",
      "Error while fetching summary for Machine learning: Page id \"machine ;earning\" does not match any pages. Try another id!\n",
      "Error while fetching summary for Deepfake: Page id \"deep face\" does not match any pages. Try another id!\n",
      "Error while fetching summary for Machine learning: Page id \"machine ;earning\" does not match any pages. Try another id!\n",
      "Error while fetching summary for Machine learning: Page id \"machine ;earning\" does not match any pages. Try another id!\n",
      "                                   Topic  \\\n",
      "0               Quantum machine learning   \n",
      "1           Adversarial machine learning   \n",
      "2            Boosting (machine learning)   \n",
      "3   Transformer (machine learning model)   \n",
      "4                          Deep learning   \n",
      "5                Artificial intelligence   \n",
      "6                 Support vector machine   \n",
      "7               Torch (machine learning)   \n",
      "8           Attention (machine learning)   \n",
      "9                          Deep learning   \n",
      "10            Deep Learning (South Park)   \n",
      "11           Deep reinforcement learning   \n",
      "12          Deep learning super sampling   \n",
      "13               Deep learning processor   \n",
      "14                            Q-learning   \n",
      "15                 Layer (deep learning)   \n",
      "16  Comparison of deep learning software   \n",
      "17             Artificial neural network   \n",
      "18                        Neural network   \n",
      "19          Convolutional neural network   \n",
      "20              Recurrent neural network   \n",
      "21           Rectifier (neural networks)   \n",
      "22               Residual neural network   \n",
      "23            Feedforward neural network   \n",
      "24                  Graph neural network   \n",
      "25                         Deep learning   \n",
      "26                Optical neural network   \n",
      "27                   Supervised learning   \n",
      "28              Self-supervised learning   \n",
      "29                      Weak supervision   \n",
      "30                 Unsupervised learning   \n",
      "31                      Feature learning   \n",
      "32                Reinforcement learning   \n",
      "33                Support vector machine   \n",
      "34                         Deep learning   \n",
      "35                Decision tree learning   \n",
      "36                 Unsupervised learning   \n",
      "37                      Feature learning   \n",
      "38              Self-supervised learning   \n",
      "39             Artificial neural network   \n",
      "40                Support vector machine   \n",
      "41                 Computational biology   \n",
      "42     Data analysis for fraud detection   \n",
      "43                         Random forest   \n",
      "44                  Competitive learning   \n",
      "\n",
      "                                              Summary  \n",
      "0   Quantum machine learning is the integration of...  \n",
      "1   Adversarial machine learning is the study of t...  \n",
      "2   In machine learning, boosting is an ensemble m...  \n",
      "3   A transformer is a deep learning architecture,...  \n",
      "4   Deep learning is the subset of machine learnin...  \n",
      "5   Artificial intelligence (AI) is the intelligen...  \n",
      "6   In machine learning, support vector machines (...  \n",
      "7   Torch is an open-source machine learning libra...  \n",
      "8   Machine learning-based attention is a mechanis...  \n",
      "9   Deep learning is the subset of machine learnin...  \n",
      "10  \"Deep Learning\" is the fourth episode of the t...  \n",
      "11  Deep reinforcement learning (deep RL) is a sub...  \n",
      "12  Deep learning super sampling (DLSS) is a famil...  \n",
      "13  A deep learning processor (DLP), or a deep lea...  \n",
      "14  Q-learning is a model-free reinforcement learn...  \n",
      "15  A layer in a deep learning model is a structur...  \n",
      "16  The following table compares notable software ...  \n",
      "17  Artificial neural networks (ANNs, also shorten...  \n",
      "18  Artificial neural networks (ANNs, also shorten...  \n",
      "19  Convolutional neural network (CNN) is a regula...  \n",
      "20  A recurrent neural network (RNN) is one of the...  \n",
      "21  In the context of artificial neural networks, ...  \n",
      "22  A Residual Neural Network (a.k.a. Residual Net...  \n",
      "23  A feedforward neural network (FNN) is one of t...  \n",
      "24  A graph neural network (GNN) is a class of art...  \n",
      "25  Deep learning is the subset of machine learnin...  \n",
      "26  An optical neural network is a physical implem...  \n",
      "27  Supervised learning (SL) is a paradigm in mach...  \n",
      "28  Self-supervised learning (SSL) is a paradigm i...  \n",
      "29  Weak supervision, also called semi-supervised ...  \n",
      "30  Supervised learning (SL) is a paradigm in mach...  \n",
      "31  In machine learning, feature learning or repre...  \n",
      "32  Reinforcement learning (RL) is an interdiscipl...  \n",
      "33  In machine learning, support vector machines (...  \n",
      "34  Deep learning is the subset of machine learnin...  \n",
      "35  Decision tree learning is a supervised learnin...  \n",
      "36  Supervised learning (SL) is a paradigm in mach...  \n",
      "37  In machine learning, feature learning or repre...  \n",
      "38  Self-supervised learning (SSL) is a paradigm i...  \n",
      "39  Artificial neural networks (ANNs, also shorten...  \n",
      "40  In machine learning, support vector machines (...  \n",
      "41  Computational biology refers to the use of dat...  \n",
      "42  Fraud represents a significant problem for gov...  \n",
      "43  Random forests or random decision forests is a...  \n",
      "44  Competitive learning is a form of unsupervised...  \n"
     ]
    }
   ],
   "source": [
    "# Import required packages\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "\n",
    "# Define the search topics\n",
    "topics = [\"Machine Learning\", \"Deep Learning\", \"Neural Networks\", \"Supervised Learning\", \"Unsupervised Learning\"]\n",
    "\n",
    "# Function to search and store results in a DataFrame\n",
    "def search_and_summarize(topics):\n",
    "    data = []\n",
    "    for topic in topics:\n",
    "        try:\n",
    "            # Search for the topic\n",
    "            search_result = wikipedia.search(topic)\n",
    "            # For each search result, get the summary\n",
    "            for item in search_result:\n",
    "                try:\n",
    "                    summary = wikipedia.summary(item)\n",
    "                    data.append({\"Topic\": item, \"Summary\": summary})\n",
    "                except Exception as e:\n",
    "                    print(f\"Error while fetching summary for {item}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while searching for {topic}: {e}\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Call the function and store results in a DataFrame\n",
    "df_topics = search_and_summarize(topics)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quantum machine learning is the integration of quantum algorithms within machine learning programs.The most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer, i.e. quantum-enhanced machine learning. While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster on a quantum computer. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data.Beyond quantum computing, the term \"quantum machine learning\" is also associated with classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments.Quantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa.Furthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as \"quantum learning theory\".\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topics.iloc[0]['Summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI generated questions on previous topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Question  \\\n",
      "0   What is the basic concept of Quantum machine l...   \n",
      "1   How is Quantum machine learning applied in rea...   \n",
      "2   What are some challenges or limitations of Qua...   \n",
      "3   What is the basic concept of Adversarial machi...   \n",
      "4   How is Adversarial machine learning applied in...   \n",
      "..                                                ...   \n",
      "92  How is Random forest applied in real-world sce...   \n",
      "93  What are some challenges or limitations of Ran...   \n",
      "94  What is the basic concept of Competitive learning   \n",
      "95  How is Competitive learning applied in real-wo...   \n",
      "96  What are some challenges or limitations of Com...   \n",
      "\n",
      "                                               Answer  \n",
      "0   Quantum machine learning is an emerging field ...  \n",
      "1   Quantum machine learning is applied in real-wo...  \n",
      "2   The primary challenges of quantum machine lear...  \n",
      "3   Adversarial machine learning focuses on unders...  \n",
      "4   Adversarial machine learning is primarily used...  \n",
      "..                                                ...  \n",
      "92  Random Forest is applied in real-world scenari...  \n",
      "93  Some challenges or limitations of Random Fores...  \n",
      "94  The basic concept of Competitive Learning invo...  \n",
      "95  Competitive learning is applied in real-world ...  \n",
      "96  Some challenges or limitations of Competitive ...  \n",
      "\n",
      "[97 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "\n",
    "data_str = \"\"\"\n",
    "Question?,Answer\n",
    "What is the basic concept of Quantum machine learning?,Quantum machine learning is an emerging field that combines quantum computing with machine learning. This approach utilizes the principles of quantum mechanics, like superposition and entanglement, to process and analyze data much faster than traditional computers. It aims to solve complex computational problems by leveraging the unique capabilities of quantum systems, potentially revolutionizing areas like data encryption, drug discovery, and complex system simulation.\n",
    "How is Quantum machine learning applied in real-world scenarios?,Quantum machine learning is applied in real-world scenarios through [application example].Quantum machine learning finds applications in various fields that require processing large datasets or complex computations. For example, in finance, it's used for optimizing portfolios and modeling market risks. In pharmaceuticals, it assists in drug discovery and molecular modeling. Quantum algorithms are also being explored for enhancing artificial intelligence capabilities and improving cybersecurity through advanced encryption methods.\n",
    "What are some challenges or limitations of Quantum machine learning?,The primary challenges of quantum machine learning include the current technological limitations of quantum computers, like qubit instability and error rates. Quantum systems are also highly sensitive to external disturbances, making them difficult to maintain and scale. Furthermore, developing algorithms that effectively leverage quantum mechanics is complex, and there's a limited pool of researchers with expertise in both quantum physics and machine learning.\n",
    "What is the basic concept of Adversarial machine learning?,Adversarial machine learning focuses on understanding and mitigating the vulnerabilities of AI models to adversarial attacks. It involves crafting inputs that cause a machine learning model to make errors. These inputs, known as adversarial examples, are slightly altered data points that deceive models into misclassifying or mispredicting. This field is crucial for improving the robustness and security of AI systems, especially in sensitive applications like autonomous vehicles and facial recognition.\n",
    "How is Adversarial machine learning applied in real-world scenarios?,Adversarial machine learning is primarily used for enhancing the security and reliability of AI systems. For example, it's employed in cybersecurity to protect against AI-driven attacks or to test the robustness of systems against manipulative inputs. In the automotive industry, it helps in testing and improving the safety of autonomous driving systems. Additionally, it's used in image and speech recognition technologies to ensure they are resilient to deceptive manipulations.\n",
    "What are some challenges or limitations of Adversarial machine learning?,The main challenges in adversarial machine learning include the difficulty in predicting and defending against all possible adversarial attacks, as attackers continuously develop new strategies. There's also a trade-off between model accuracy and robustness; making a model more secure against adversarial attacks can reduce its performance on standard inputs. Additionally, generating and testing against adversarial examples can be computationally expensive.\n",
    "What is the basic concept of Boosting (machine learning)?,The basic concept of Boosting in machine learning involves creating a highly accurate prediction rule by combining many weak and inaccurate rules. This is done iteratively, with each subsequent model focusing on the data points that were misclassified by the previous models. The idea is to give more weight to the difficult cases in each round, making the models adapt to the complexity of the data progressively. Boosting techniques such as AdaBoost or Gradient Boosting are popular examples.\n",
    "How is Boosting (machine learning) applied in real-world scenarios?,Boosting is applied in real-world scenarios through various applications, such as: 1) Credit Scoring: Financial institutions use boosting algorithms to predict the likelihood that a given loan applicant will default. By combining weak predictors into a strong one, they can make more accurate decisions. 2) Medical Diagnosis: Boosting can help in combining simple diagnostic rules to improve the detection of diseases from complex patterns in patient data. 3) Fraud Detection: In the realm of cybersecurity, boosting is used to combine simple anomaly detection rules to improve the identification of fraudulent activities within large-scale transactions.\n",
    "What are some challenges or limitations of Boosting (machine learning)?,Some challenges or limitations of Boosting include: 1) Overfitting: If not properly managed, boosting can overfit to the noise in the training data, especially when the data is noisy and complex. 2) Computationally Intensive: Boosting, by its iterative nature, can be computationally intensive and slow to train, especially with large datasets. 3) Difficulty with Noisy Data: Boosting algorithms are sensitive to outliers because they focus on correcting misclassified points, which can be problematic if the misclassifications are due to noise.\n",
    "What is the basic concept of Transformer (machine learning model)?,The basic concept of the Transformer model involves using self-attention mechanisms to weigh the influence of different parts of the input data. Unlike previous sequence-to-sequence models that use recurrent neural networks (RNNs), Transformers process entire sequences of data simultaneously, making them highly parallelizable and efficient. The Transformer architecture, introduced in the paper \"Attention is All You Need,\" has become a foundation for many state-of-the-art natural language processing (NLP) models.\n",
    "How is Transformer (machine learning model) applied in real-world scenarios?,Transformers are applied in real-world scenarios through: 1) Machine Translation: They are at the core of modern machine translation services, providing rapid and accurate translations between languages. 2) Text Summarization: Transformers can generate concise summaries of long documents, which is useful in areas such as legal document analysis or condensing news articles. 3) Question Answering Systems: These models can understand and respond to natural language queries in applications like virtual assistants and customer support bots.\n",
    "What are some challenges or limitations of Transformer (machine learning model)?,Some challenges or limitations of Transformer models include: 1) Resource Intensive: They require significant computational resources for training, often necessitating powerful GPUs or TPUs. 2) Data Hungry: Transformers generally require large amounts of training data to perform well, which can be a barrier for less-resourced languages or specialized domains. 3) Interpretability: The complex nature of Transformer models makes them less interpretable, which can be an issue in applications where understanding the decision-making process is crucial.\n",
    "What is the basic concept of Deep learning?,The basic concept of Deep learning involves neural networks with multiple layers that enable the learning of data representations at varying levels of abstraction. These layers are composed of nodes, or \"neurons,\" each of which applies simple computations to the input data. Through the network, these computations become increasingly complex, allowing the model to learn to perform tasks like image and speech recognition by identifying patterns in the data.\n",
    "How is Deep learning applied in real-world scenarios?,Deep learning is applied in real-world scenarios through applications such as:Image Recognition: Used in photo tagging on social media and medical imaging to identify diseases.Speech Recognition: Powers virtual assistants like Siri and Alexa, as well as transcription services.Autonomous Vehicles: Enables cars to recognize objects, make decisions, and navigate roads.\"What are some challenges or limitations of Deep learning?,Some challenges or limitations of Deep learning include:Data Requirements: Requires large amounts of labeled data for training.Computational Resources: Needs substantial computational power, especially GPUs, for processing.Interpretability: Deep learning models, especially deep neural networks, are often referred to as \"\"black boxes\"\" due to their lack of interpretability.\"\n",
    "What is the basic concept of Artificial intelligence?,The basic concept of Artificial intelligence involves machines designed to perform tasks that would typically require human intelligence. This includes problem-solving, pattern recognition, learning from data, and understanding natural language.\n",
    "How is Artificial intelligence applied in real-world scenarios?,Artificial intelligence is applied in real-world scenarios through applications such as:Customer Service: AI chatbots and virtual assistants handle customer inquiries and services.Healthcare: AI algorithms assist in diagnosis, personalized medicine, and patient monitoring.Manufacturing: AI optimizes production lines, improves supply chain logistics, and maintains equipment.\"\n",
    "What are some challenges or limitations of Artificial intelligence?,Some challenges or limitations of Artificial intelligence include:Ethical Concerns: AI raises questions about privacy, surveillance, and decision-making without human oversight.Job Displacement: Automation through AI could lead to displacement of workers in certain sectors.Bias: AI systems can inherit and amplify biases present in their training data or algorithms.\"\n",
    "What is the basic concept of Support vector machine?,The basic concept of Support vector machine involves a supervised learning model used for classification and regression tasks. It works by finding the hyperplane that best divides a dataset into classes. The support vectors are the data points closest to the hyperplane, and the goal of the SVM is to maximize the margin between these points and the hyperplane.\n",
    "How is Support vector machine applied in real-world scenarios?,Support vector machine is applied in real-world scenarios through applications such as:Bioinformatics: Used for protein classification and cancer classification based on gene expression data.Image Classification: SVMs help in classifying images in various applications, including handwriting recognition.Market Forecasting: SVMs are used in the financial industry to predict movements in stock markets.\"\n",
    "what types of neural networks exist?,There are several types of neural networks available such as feed-forward neural network, Radial Basis Function (RBF) Neural Network, Multilayer Perceptron, Convolutional Neural Network, Recurrent Neural Network(RNN), Modular Neural Network and Sequence to sequence models. Each of the neural network types is specific to certain business scenarios and data patterns.\n",
    "What are some challenges or limitations of Support vector machine?,Some challenges or limitations of Support vector machine include:Scalability: SVMs can become impractical when the dataset is very large.Kernel Selection: The choice of the kernel and its parameters can greatly influence the performance of the SVM, and there is no guaranteed way to choose the best one without experimentation.Multiclass Classification: SVMs are inherently binary classifiers, and thus more complex methods are required for multiclass problems.\"\n",
    "What is the basic concept of Torch (machine learning)?,The basic concept of Torch (machine learning) involves a scientific computing framework with wide support for machine learning algorithms. It is a Lua-based scripting language coupled with a C/CUDA backend. Torch provides flexibility and speed in building machine learning models, owing to its efficient GPU support and easy-to-use neural network library.\n",
    "How is Torch (machine learning) applied in real-world scenarios?,Torch (machine learning) is applied in real-world scenarios through applications such as:Computer Vision: Torch has been used for tasks like image classification, object detection, and facial recognition systems.Natural Language Processing: Companies leverage Torch for language modeling and translation services.Research: It's widely used in academia and industry for developing deep learning models due to its flexibility and efficiency.\"\n",
    "What are some challenges or limitations of Torch (machine learning)?,Some challenges or limitations of Torch (machine learning) include:Popularity and Community Size: Torch has a smaller community compared to other frameworks like TensorFlow or PyTorch, which can result in fewer resources and support options.Language Barrier: Being Lua-based, it is not as widely adopted by the machine learning community, which tends to prefer Python.Transition to PyTorch: With the development of PyTorch, which is inspired by Torch but uses Python, the focus has shifted away from Torch, resulting in less active development.\"\n",
    "What is the basic concept of Attention (machine learning)?,The basic concept of Attention (machine learning) involves selectively focusing on certain parts of the input data and giving different parts of the data different levels of importance when processing it. In the context of neural networks, particularly in sequence-to-sequence tasks, it allows the model to dynamically weigh the influence of different input parts to produce a more accurate output.\n",
    "How is Attention (machine learning) applied in real-world scenarios?,Attention (machine learning) is applied in real-world scenarios through applications such as:Machine Translation: It helps models to focus on relevant parts of a sentence when translating it into another language.Text Summarization: Attention mechanisms can identify the most informative parts of a document to generate concise summaries.Speech Recognition: These mechanisms allow models to emphasize the more relevant parts of an audio signal to improve transcription accuracy.\"\n",
    "What are some challenges or limitations of Attention (machine learning)?,Some challenges or limitations of Attention (machine learning) include:Complexity: Attention mechanisms can add complexity to the model, which may increase the computational resources required.Interpretability: Although attention weights can offer some insights, the reasons behind these weights can still be difficult to interpret.Long Sequences: While attention helps with long sequences, extremely long inputs can still pose challenges in terms of computational efficiency and memory usage.\"\n",
    "What is the basic concept of Deep reinforcement learning?,The basic concept of Deep reinforcement learning involves combining deep neural networks with a reinforcement learning architecture that enables agents to learn optimal actions through trial-and-error interactions with an environment. It uses deep learning to approximate the value functions or policies, allowing agents to make decisions in high-dimensional spaces that would be intractable with traditional reinforcement learning techniques.\n",
    "How is Deep reinforcement learning applied in real-world scenarios?,Deep reinforcement learning is applied in real-world scenarios through applications such as:Gaming: Creating agents that can play complex video games at a superhuman level.Robotics: Teaching robots to perform tasks like walking, picking up items, or performing surgery.Resource Management: Optimizing resources in logistics, data centers, and energy grids.\"\n",
    "What are some challenges or limitations of Deep reinforcement learning?,Some challenges or limitations of Deep reinforcement learning include:Sample Inefficiency: Requires a large number of interactions with the environment, which can be impractical.Stability and Robustness: Learning can be unstable and sensitive to hyperparameters and the initial setup.Simulation to Reality Transfer: Policies trained in simulated environments may not perform well in the real world due to differences between the simulation and reality.\"\n",
    "What is the basic concept of Deep learning super sampling?,The basic concept of Deep learning super sampling (DLSS) involves using deep learning to upscale lower-resolution images in real-time to higher resolutions. It aims to produce images that look like they were rendered in high resolution, thus saving on computational resources while achieving similar visual fidelity.\n",
    "How is Deep learning super sampling applied in real-world scenarios?,Deep learning super sampling is applied in real-world scenarios through applications such as:Video Gaming: Enabling smoother and higher-quality graphics performance in video games without requiring extremely powerful hardware.Virtual Reality: Improving image quality in VR headsets without increasing the demand on GPUs.Content Creation: Assisting in faster rendering of high-resolution images and animations for digital content creation.\"\n",
    "What are some challenges or limitations of Deep learning super sampling?,Some challenges or limitations of Deep learning super sampling include:Hardware Requirements: Often requires specific, usually high-end, graphics cards that support the technology.Quality Variation: The quality of the upscaled image might not always match the quality of a natively high-resolution image.Support: Not all applications or games support DLSS, limiting its usage to those that do.\"\n",
    "What is the basic concept of Deep learning processor?,The basic concept of a Deep learning processor involves specialized hardware designed to efficiently handle the computational demands of deep learning tasks. These processors are optimized for matrix and vector operations, which are fundamental to neural network computations, and they often include capabilities to accelerate training and inference.\n",
    "How is Deep learning processor applied in real-world scenarios?,Deep learning processors are applied in real-world scenarios through applications such as:Smartphones: Enhancing features like camera image processing, facial recognition, and augmented reality.Automotive: Powering advanced driver-assistance systems (ADAS) and autonomous vehicle computations.Data Centers: Accelerating the computation in servers for tasks like large-scale machine learning model training and inference.\"\n",
    "What are some challenges or limitations of Deep learning processor?,Some challenges or limitations of Deep learning processors include:Cost: Developing and manufacturing specialized processors can be expensive.Energy Consumption: High-performance deep learning tasks can be energy-intensive, even with efficient processors.Compatibility: Integrating deep learning processors with existing systems and software frameworks can require significant engineering effort.\"What is the basic concept of Q-learning?,The basic concept of Q-learning involves a model-free reinforcement learning algorithm to learn the quality of actions, telling an agent what action to take under what circumstances. It doesn't require a model of the environment and can handle problems with stochastic transitions and rewards, without needing adaptations.\n",
    "How is Q-learning applied in real-world scenarios?,Q-learning is applied in real-world scenarios through applications such as:Robotics: Teaching robots to perform tasks like navigation and manipulation by trial and error.Game Playing: Training AI to play games like chess or Go where the set of possible actions and states can be large.Automated Trading: Developing financial strategies by learning to predict stock price actions and trading accordingly.\"\n",
    "What are some challenges or limitations of Q-learning?,Some challenges or limitations of Q-learning include:Scalability: It can struggle with very large state spaces due to the requirement to store Q-values for every state-action pair.Convergence Time: It can take a long time to converge to the optimal action-value function, especially in environments with many states and actions.Overestimation of Q-values: Due to the max operation in the Q-learning update, it can lead to an overestimation of the action values.\"\n",
    "What is the basic concept of Layer (deep learning)?,The basic concept of a Layer in deep learning involves a collection of neurons that operates on input data or the output of previous layers in a neural network. Each layer applies a set of functions, such as linear transformations followed by a non-linear activation function, to transform the data and extract features.\n",
    "How is Layer (deep learning) applied in real-world scenarios?,Layers in deep learning are applied in real-world scenarios through:Image Recognition: Convolutional layers are used to process pixel data and identify features in images.Speech Recognition: Recurrent layers process time-series data for tasks like speech-to-text.Natural Language Processing: Transformer layers help in understanding the context in text data for tasks like translation and sentiment analysis.\"\n",
    "What are some challenges or limitations of Layer (deep learning)?,Some challenges or limitations of layers in deep learning include:Vanishing/Exploding Gradients: In very deep networks, gradients can become very small or large, making the network hard to train.Computational Resources: More complex layers, such as convolutional layers, require significant computation power.Design Complexity: Designing the right architecture, choosing the number of layers and their types, is a non-trivial task that often requires extensive experimentation.\"\n",
    "What deep learning software is the best?,Determining the \"best\" deep learning software depends on the specific needs and preferences of the user, including ease of use, flexibility, performance, and community support. Popular frameworks include TensorFlow, PyTorch, Keras (now integrated with TensorFlow), and Microsoft's Cognitive Toolkit (CNTK). Each has its strengths and weaknesses, and the choice can depend on the specific application and the user's familiarity with the framework.\n",
    "What is the basic concept of Artificial neural network?,The basic concept of an Artificial neural network involves a computational model inspired by the human brain's network of neurons. It consists of interconnected groups of nodes or neurons that process information using a connectionist approach to computation. In most cases, neural networks are an adaptive system that changes its structure based on external or internal information that flows through the network during the learning phase.\n",
    "How is Artificial neural network applied in real-world scenarios?,Artificial neural networks are applied in real-world scenarios through:Credit Scoring: Evaluating the risk of lending to individuals.Medical Diagnosis: Assisting doctors in diagnosing diseases based on symptoms and test results.Stock Market Prediction: Predicting stock price movements and making investment decisions.\"\n",
    "What are some challenges or limitations of Artificial neural network?,Some challenges or limitations of Artificial neural networks include:Data Dependency: They require large amounts of data to perform well.Black Box Nature: The decision-making process can be opaque and difficult to interpret.Overfitting: Without proper regularization, neural networks can overfit to the training data, making them perform poorly on unseen data.\"\n",
    "What is the basic concept of Convolutional neural network?,The basic concept of a Convolutional Neural Network (CNN) involves a deep learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image, and be able to differentiate one from the other. The pre-processing required in a CNN is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, CNNs have the ability to learn these filters/characteristics.\n",
    "How is Convolutional neural network applied in real-world scenarios?,A Convolutional Neural Network is applied in real-world scenarios through applications such as:Image Classification: Identifying objects within an image, which is used in photo tagging technologies.Medical Image Analysis: Helping to diagnose diseases by analyzing medical scans.Autonomous Vehicles: Interpreting the surrounding environment to help vehicles navigate safely.\"\n",
    "What are some challenges or limitations of Convolutional neural network?,Some challenges or limitations of Convolutional Neural Networks include:High Computational Cost: CNNs require significant computational resources for training and inference, especially with deep architectures and large datasets.Overfitting: Without proper regularization, they can overfit to the training data and may not generalize well to new, unseen data.Transparency: CNNs, like other deep networks, are often considered black boxes, and their decision-making process can be difficult to interpret.\"\n",
    "What is the basic concept of Recurrent neural network?,The basic concept of a Recurrent Neural Network (RNN) involves neural networks that are designed to recognize patterns in sequences of data, such as numerical time series data emanating from sensors, stock markets, and government agencies (but also including text, genomes, handwriting, and the spoken word). The idea is to have neurons that fire for a duration of time, as a way of having memory about previous data points, influencing the decision about current data points.\n",
    "How is Recurrent neural network applied in real-world scenarios?,Recurrent Neural Networks are applied in real-world scenarios through:Language Translation: Translating text or speech from one language to another.Speech Recognition: Converting spoken words into text.Time Series Prediction: Predicting stock prices, weather forecasting, and any other sequence-dependent data.\"\n",
    "What are some challenges or limitations of Recurrent neural network?,Some challenges or limitations of Recurrent Neural Networks include:Difficulty with Long-Term Dependencies: Traditional RNNs struggle to carry information across many time steps, leading to the vanishing or exploding gradient problems.Training Time: They can be slow to train, often requiring many epochs to converge due to the sequential nature of the data.Resource Intensive: RNNs require significant computational resources for training complex models.\"\n",
    "What is the basic concept of Rectifier (neural networks)?,The basic concept of a Rectifier in neural networks involves a type of activation function that is defined as the positive part of its argument. The rectifier activation function, commonly referred to as ReLU (Rectified Linear Unit), allows models to account for nonlinearity in the data by passing through positive values while zeroing out negative values.\n",
    "How is Rectifier (neural networks) applied in real-world scenarios?,Rectifiers (neural networks) are applied in real-world scenarios throughDeep Learning Models: Almost any deep learning network leverages ReLU or one of its variants for hidden layers, due to its simplicity and efficiency.Computer Vision: CNNs commonly use ReLU to introduce nonlinearity after convolutional layers.Speech and Audio Processing: Used in models for processing and recognizing audio signals.\"\n",
    "What are some challenges or limitations of Rectifier (neural networks)?,Some challenges or limitations of Rectifier (neural networks) include:Dying ReLU Problem: When a large number of activations become zero, the gradient can also become zero, leading to dead neurons that do not activate across any data points.Non-zero Centric: Since ReLU is not zero-centered, this can sometimes lead to gradient descent oscillations during optimization.Fragility During Training: If the learning rate is set too high, it is possible for the weights to update in such a way that the neuron will never activate on any data point again.\"\n",
    "What is the basic concept of Residual neural network?,The basic concept of Residual Neural Network (ResNet) involves the integration of skip connections (or shortcut connections) that bypass one or more layers. Traditional neural networks try to learn mappings from input to output, while ResNets learn residual functions with reference to the layer inputs. This approach eases the training of very deep networks by enabling the direct propagation of gradients through the skip connections.\n",
    "How is Residual neural network applied in real-world scenarios?,Residual Neural Networks are applied in real-world scenarios through:Advanced Image Recognition: Being used in systems where high accuracy of image classification is paramount, like automated medical diagnosis from images.Object Detection: Used in surveillance systems to detect and classify objects in real-time.Visual Effects: Assisting in high-resolution image synthesis and editing, including the creation of visual effects in movies and games.\"\n",
    "What are some challenges or limitations of Residual neural network?,Some challenges or limitations of Residual Neural Networks include:Computational Resources: Despite easing training for deep networks, ResNets still require substantial computation and memory, especially as the network depth increases.Optimization Difficulties: As the network gets very deep, even with skip connections, it can be difficult to optimize due to the complexity of the model.Diminishing Feature Reuse: There is some evidence to suggest that the features reused across layers may diminish in later layers of very deep networks.\"\n",
    "What is the basic concept of Feedforward neural network?,The basic concept of a Feedforward Neural Network involves a straightforward neural network where connections between the nodes do not form a cycle. This type of network is called feedforward because the information only moves forward from the input nodes, through the hidden nodes (if any), and finally to the output nodes. There are no cycles or loops in the network.\n",
    "How is Feedforward neural network applied in real-world scenarios?,Feedforward Neural Networks are applied in real-world scenarios through:Pattern Recognition: Such as recognizing the types of objects in images or the patterns of customer behavior.Data Classification: Classifying emails into spam and non-spam categories.Regression Analysis: Predicting numerical values like house prices or stock market indices.\"\n",
    "What are some challenges or limitations of Feedforward neural network?,Some challenges or limitations of Feedforward Neural Networks include:Limited Complexity: They are not suitable for problems that require modeling sequential or time-series data due to the lack of feedback loops.Overfitting: Without adequate regularization, they can overfit to training data, performing poorly on unseen data.Expressiveness: There might be limitations on the kinds of functions they can represent, which can be mitigated by adding more layers or nodes, at the cost of increased complexity.\"\n",
    "What is the basic concept of Graph neural network?,The basic concept of a Graph Neural Network (GNN) involves neural networks that operate on graph structures. They are designed to capture the dependency of graphs via message passing between the nodes of graphs. By doing so, GNNs can incorporate information about the graph structure into the learning process, which is essential for problems where the data is naturally represented as a graph (e.g., social networks, molecular structures).\n",
    "How is Graph neural network applied in real-world scenarios?,Graph Neural Networks are applied in real-world scenarios through:Social Network Analysis: For recommendations and detecting communities or abnormal behaviors.Chemoinformatics: Predicting the properties of molecules and drugs.Traffic Networks: Optimizing routes and predicting congestion in transport networks.\"\n",
    "What are some challenges or limitations of Graph neural network?,Some challenges or limitations of Graph Neural Networks include:Scalability: Processing large graphs can be computationally demanding.Dynamic Graphs: Many GNN models are not designed to handle graphs that change over time.Over-smoothing: As the number of layers increases, node representations may become too similar, leading to a loss of useful information for downstream tasks.\"\n",
    "What is the basic concept of Optical neural network?,The basic concept of Optical Neural Networks (ONNs) involves the use of optical phenomena to perform neural computations. These networks utilize light rather than electrical signals for processing, taking advantage of the inherent parallelism and high-speed nature of optics. ONNs can implement various neural network architectures through optical components like lenses, mirrors, and spatial light modulators.\n",
    "How is Optical neural network applied in real-world scenarios?,Optical Neural Networks are applied in real-world scenarios primarily in areas where the high-speed computation of traditional neural networks is a bottleneck, such as:High-Speed Image Processing: For tasks like real-time image classification and pattern recognition, where the speed of light can facilitate faster processing than electronic systems.Signal Processing: In telecommunications for rapid processing of optical signals without conversion to electrical signals.\"\n",
    "What are some challenges or limitations of Optical neural network?,Some challenges or limitations of Optical Neural Networks include:Hardware Complexity: Designing and building optical components for neural computation can be complex and expensive.Scalability: While optical components can offer high speeds, scaling them to the size and complexity of electronic neural networks is challenging.Integration: Integrating ONNs with existing electronic systems and data infrastructure can be technically challenging.\"\n",
    "What is the basic concept of Supervised learning?,The basic concept of Supervised Learning involves training a model on a labeled dataset, which means that each training example is paired with an output label. The model learns to make predictions based on this input-output mapping and can then apply this learned function to new, unseen data. It is called supervised because the process of an algorithm learning from the training dataset can be thought of as a teacher supervising the learning process.\n",
    "How is Supervised learning applied in real-world scenarios?,Supervised Learning is applied in real-world scenarios through:Spam Detection: In email services, to classify messages as spam or not spam.Medical Diagnosis: To predict the presence or absence of diseases based on patient data.Financial Fraud Detection: To identify fraudulent transactions based on historical fraud patterns.\"\n",
    "What are some challenges or limitations of Supervised learning?,Some challenges or limitations of Supervised Learning include:Data Annotation: Requires a large amount of labeled data, which can be expensive and time-consuming to produce.Generalization: Models may not generalize well to unseen data, especially if the training data is not representative of the real-world distribution.Overfitting: There is a risk of overfitting to the training data, which means that the model learns the noise in the data rather than the underlying pattern.\"\n",
    "What is the basic concept of Self-supervised learning?,The basic concept of Self-supervised Learning involves a type of unsupervised learning where the data provides the supervision. The model is trained to predict any part of its input from any other part, by automatically generating labels from the input data itself. This can involve tasks like predicting the next word in a sentence or the next frame in a video sequence.\n",
    "How is Self-supervised learning applied in real-world scenarios?,Self-supervised Learning is applied in real-world scenarios through:Natural Language Processing: For language models that can predict the next word in a sequence, improving machine translation, text summarization, and question-answering systems.Robotics: Allowing robots to understand and interact with the world around them by learning from unlabeled sensory data.Predictive Maintenance: In manufacturing, predicting machine failures from equipment sensor data.\"\n",
    "What are some challenges or limitations of Self-supervised learning?,Some challenges or limitations of Self-supervised Learning include:Quality of Self-generated Labels: The quality of learning is highly dependent on how well the self-generated labels represent the underlying structure of the data.Complexity of Task Design: Designing self-supervised tasks that are meaningful and contribute to the learning of useful representations can be non-trivial.Dependence on Data Quantity: Although it does not require labeled data, self-supervised learning often relies on large quantities of data to learn effectively.\"\n",
    "What is the basic concept of Weak supervision?,The basic concept of Weak Supervision involves using noisy, limited, or imprecise sources to provide a signal for model training in lieu of a fully labeled training set. This method is especially useful when it's too costly or time-consuming to obtain a large set of labeled data. The idea is to use whatever information is available — which may include heuristic rules, crowd-sourced labels, or data from related tasks — to generate training signals.\n",
    "How is Weak supervision applied in real-world scenarios?,Weak supervision is applied in real-world scenarios through:Data Labeling: Utilizing user-generated tags or keywords for training image recognition or natural language processing models.Medical Diagnoses: Employing noisy labels from various diagnostic tools when consolidated, expert-labeled datasets are not available.Sentiment Analysis: Leveraging emoticons or certain keywords as proxies for sentiment labels in text data.\"\n",
    "What are some challenges or limitations of Weak supervision?,Some challenges or limitations of Weak supervision include:Noise in Labels: The labels derived from weak supervision sources can be noisy, which can lead to decreased model performance.Bias: Models trained with weakly supervised data can inherit biases present in the heuristic rules or noisy labels.Complexity in Integration: Combining multiple weak supervision sources can be complex and often requires sophisticated models to handle the noise and contradictions in the training data.\"\n",
    "What is the basic concept of Unsupervised learning?,The basic concept of Unsupervised Learning involves algorithms that learn patterns from untagged data. The system tries to learn the structure of the data without any explicit instructions about what patterns to find. It is often used to find hidden patterns or groupings in data, and is key in tasks like clustering, association, and dimensionality reduction.\n",
    "How is Unsupervised learning applied in real-world scenarios?,Unsupervised learning is applied in real-world scenarios through:Market Basket Analysis: Identifying products frequently bought together and using this information for marketing strategies or store layouts.Clustering: Grouping customers with similar behaviors for targeted marketing campaigns.Anomaly Detection: Identifying unusual patterns that could indicate fraudulent activity in finance or fault detection in manufacturing systems.\"\n",
    "What are some challenges or limitations of Unsupervised learning?,Some challenges or limitations of Unsupervised Learning include:Interpretability: The patterns or groupings found by unsupervised learning algorithms can sometimes be difficult to interpret and validate.Dependency on Data Quality: The success of unsupervised learning heavily relies on the quality of the data; poor quality data can lead to misleading patterns.Lack of Objective Evaluation: Without predefined labels, it can be challenging to objectively assess the performance of an unsupervised learning model.\"\n",
    "What is the basic concept of Feature learning?,The basic concept of Feature Learning involves algorithms learning to automatically discover the representations needed for feature detection or classification from raw data. This learning can be supervised, semi-supervised or unsupervised. Feature learning is important for improving the efficiency of machine learning models by identifying the most informative features from the input data.\n",
    "How is Feature learning applied in real-world scenarios?,Feature learning is applied in real-world scenarios throughComputer Vision: Automatically identifying features in images that are most relevant for tasks like object recognition.Speech Recognition: Learning features from raw audio signals that can distinguish between different words or phonemes.Predictive Maintenance: Detecting features in sensor data that precede equipment failures.\"\n",
    "What are some challenges or limitations of Feature learning?,Some challenges or limitations of Feature Learning include:Computational Cost: Learning features from very large datasets can be computationally expensive and time-consuming.Overfitting: If not regulated, feature learning can overfit to the training dataset, leading to poor generalization to new data.Selection of Model Architecture: The architecture of the learning model can greatly influence the quality of the learned features, and there is often no clear rule on how to select the best architecture for a given task.\"\n",
    "What is the basic concept of Reinforcement learning?,The basic concept of Reinforcement Learning (RL) involves an agent learning to make decisions by performing actions in an environment to achieve some notion of cumulative reward. The agent learns from the consequences of its actions, rather than from being told explicitly what to do, through a process of trial and error. Reinforcement learning is defined by three primary components: a policy (the strategy that the agent employs), a reward signal (the goal of the agent), and a value function (which determines the quality of the states an agent might visit).\n",
    "How is Reinforcement learning applied in real-world scenarios?,Reinforcement Learning is applied in real-world scenarios through:Autonomous Vehicles: Training self-driving cars to make decisions like lane changing and avoiding obstacles.Robotics: Teaching robots to perform tasks that require a sequence of movements, such as assembling products or performing surgery.Game Playing: Developing AI that can play games such as chess, Go, or video games at a high or superhuman level.\"\n",
    "What are some challenges or limitations of Reinforcement learning?,Some challenges or limitations of Reinforcement Learning include:Sample Efficiency: RL algorithms often require a large number of samples to learn effectively, which can be expensive or impractical in real-world situations.Stability and Convergence: RL can be unstable and sensitive to the initial conditions, hyperparameters, and even the random seed used in the stochastic processes.Exploration vs. Exploitation: Balancing the need to explore the environment to learn about it and exploiting known information to maximize the reward is a non-trivial problem.\"\n",
    "What is the basic concept of Decision tree learning?,The basic concept of Decision Tree Learning involves a predictive modeling approach that maps observations about an item to conclusions about the item's target value. It is a type of supervised learning algorithm (having a pre-defined target variable) that is used for classification and regression. A decision tree is built by recursively partitioning the input space, and making a decision at each node based on a single feature.\n",
    "How is Decision tree learning applied in real-world scenarios?,Decision Tree Learning is applied in real-world scenarios through:Credit Scoring: Assessing a borrower's risk by analyzing their financial history and making lending decisions.Medical Diagnosis: Assisting clinicians in diagnosing diseases based on symptoms and test results.Customer Segmentation: Dividing customers into groups based on purchasing behavior or demographics for targeted marketing.\"\n",
    "What are some challenges or limitations of Decision tree learning?,Some challenges or limitations of Decision Tree Learning include:Overfitting: Trees can easily overfit to the training data if they are allowed to grow complex without constraints.Instability: Small changes in the data can lead to a completely different tree being generated.Bias: Decision trees are biased toward splits on features with more levels, which can affect the accuracy of the model.\"\n",
    "What is the basic concept of Computational biology?,The basic concept of Computational Biology involves the application of computational techniques to understand and model the structures, functions, and interactions of complex biological systems. It integrates principles from biology with computer science, statistics, mathematics, and engineering to analyze and interpret biological data.\n",
    "How is Computational biology applied in real-world scenarios?,Computational Biology is applied in real-world scenarios through:Genomics: Analyzing sequences of DNA to discover gene functions and structures or to trace the evolutionary history of organisms.Drug Discovery: Using computational methods to identify potential drug candidates and to predict their effects.Protein Structure Prediction: Determining the three-dimensional shape of proteins, which is crucial for understanding their function.\"\n",
    "What are some challenges or limitations of Computational biology?,Some challenges or limitations of Computational Biology include:Data Complexity: Biological data is often high-dimensional, noisy, and heterogeneous, which makes it challenging to analyze and interpret.Computational Resources: Many computational biology methods require significant computational power and storage.Interdisciplinary Expertise: Effectively working in computational biology often requires a strong understanding of both biological sciences and computational methods.\"\n",
    "What is the basic concept of Data analysis for fraud detection?,The basic concept of Data analysis for fraud detection involves using statistical techniques, machine learning algorithms, and data analytics tools to identify patterns and anomalies in data that may indicate fraudulent activity. The goal is to recognize unusual behavior that deviates from the norm, which could suggest attempts at deception, theft, or other illegal activities.\n",
    "How is Data analysis for fraud detection applied in real-world scenarios?,Data analysis for fraud detection is applied in real-world scenarios through:Banking and Finance: Monitoring transactions to spot unusual patterns that may indicate credit card fraud or money laundering.Insurance: Detecting fraudulent claims by analyzing discrepancies in claim reports compared to the normative patterns.E-commerce: Identifying fake reviews or transactions to prevent retail fraud.\"\n",
    "What are some challenges or limitations of Data analysis for fraud detection?,Some challenges or limitations of Data analysis for fraud detection include:Evolving Tactics: Fraudsters constantly change their strategies, making it difficult to keep up with new types of fraud.False Positives: Discriminating between legitimate and fraudulent activity can be challenging and can result in false positives, where legitimate transactions are flagged as fraudulent.Data Privacy: Collecting and analyzing data for fraud detection must be balanced with respecting user privacy and complying with data protection regulations.\"\n",
    "What is the basic concept of Random forest?,The basic concept of Random Forest involves an ensemble learning method for classification, regression, and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. It combines the simplicity of decision trees with flexibility, resulting in a model that has higher accuracy without a significant increase in complexity.\n",
    "How is Random forest applied in real-world scenarios?,Random Forest is applied in real-world scenarios through:Medical Diagnosis: Predicting patient outcomes based on clinical data.Stock Market Prediction: Analyzing the stock market trends and predicting future stock values.Quality Assessment: Evaluating the quality of products in manufacturing based on various features.\"\n",
    "What are some challenges or limitations of Random forest?,Some challenges or limitations of Random Forest include:Performance: While usually more accurate than individual decision trees, random forests can still be outperformed by other models, especially on tasks where data relationships are highly non-linear.Interpretability: Individual decision trees are easy to interpret, but a forest with hundreds of trees can be difficult to interpret.Resource Intensive: Training a large number of trees can be computationally intensive and require substantial memory, making random forests less practical for very large datasets or real-time applications.\"\n",
    "What is the basic concept of Competitive learning?,The basic concept of Competitive Learning involves a form of unsupervised learning where neurons in a neural network compete with each other to become activated. In this learning paradigm, only one neuron or a group of neurons is activated at a time, often referred to as the \"winner-takes-all\" approach. This method is used for clustering and feature mapping, where the network learns to recognize groups of similar input patterns.\n",
    "How is Competitive learning applied in real-world scenarios?,Competitive learning is applied in real-world scenarios throughClustering: Grouping similar data points together in data mining tasks.Vector Quantization: In signal processing for data compression.Pattern Recognition: Recognizing patterns in complex datasets, like identifying similar customer behaviors.\"\n",
    "What are some challenges or limitations of Competitive learning?,Some challenges or limitations of Competitive learning include:Scalability: It can be challenging to scale competitive learning algorithms to very large datasets.Dependency on Initialization: The results can be highly dependent on the initial state of the network.Convergence Issues: The learning process can be slow, and there is no guarantee that it will converge to a useful solution, especially for complex or non-linearly separable data sets.\"\n",
    "\"\"\"\n",
    "\n",
    "# Create a DataFrame from the string\n",
    "s = data_str.replace(\"?,\", \"&\")\n",
    "# Create a DataFrame from the string\n",
    "df = pd.read_csv(\n",
    "    StringIO(s),\n",
    "    sep=\"&\",\n",
    "    on_bad_lines=\"skip\",\n",
    ")\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interiew questions on Iternet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Question  \\\n",
      "0    How machine learning is different from general...   \n",
      "1    What are some real-life applications of cluste...   \n",
      "2         How to choose an optimal number of clusters?   \n",
      "3    What is feature engineering? How does it affec...   \n",
      "4            What is a Hypothesis in Machine Learning?   \n",
      "..                                                 ...   \n",
      "104                     Explain how a ROC curve works.   \n",
      "105  What’s the difference between Type I and Type ...   \n",
      "106  how does deep learning contrast with other mac...   \n",
      "107    What’s the “kernel trick” and how is it useful?   \n",
      "108  How do you handle missing or corrupted data in...   \n",
      "\n",
      "                                                Answer  \n",
      "0    In general programming, we have the data and t...  \n",
      "1    The clustering technique can be used in multip...  \n",
      "2    By using the Elbow method we decide an optimal...  \n",
      "3    Feature engineering refers to developing some ...  \n",
      "4    A hypothesis is a term that is generally used ...  \n",
      "..                                                 ...  \n",
      "104  The ROC curve is a graphical representation of...  \n",
      "105  Type I error is a false positive, while Type I...  \n",
      "106  Deep learning is a subset of machine learning ...  \n",
      "107  he Kernel trick involves kernel functions that...  \n",
      "108  You could find missing/corrupted data in a dat...  \n",
      "\n",
      "[109 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data: list of tuples (question, answer)\n",
    "data = [\n",
    "    (\"How machine learning is different from general programming?\", \n",
    "     \"In general programming, we have the data and the logic by using these two we create the answers. But in machine learning, we have the data and the answers and we let the machine learn the logic from them so, that the same logic can be used to answer the questions which will be faced in the future. Also, there are times when writing logic in codes is not possible so, at those times machine learning becomes a savior and learns the logic itself.\"),\n",
    "    \n",
    "    (\"What are some real-life applications of clustering algorithms?\", \n",
    "     \"The clustering technique can be used in multiple domains of data science like image classification, customer segmentation, and recommendation engine. One of the most common use is in market research and customer segmentation which is then utilized to target a particular market group to expand the businesses and profitable outcomes.\"),\n",
    "    \n",
    "    (\"How to choose an optimal number of clusters?\", \n",
    "     \"By using the Elbow method we decide an optimal number of clusters that our clustering algorithm must try to form. The main principle behind this method is that if we will increase the number of clusters the error value will decrease. But after an optimal number of features, the decrease in the error value is insignificant so, at the point after which this starts to happen, we choose that point as the optimal number of clusters that the algorithm will try to form.\"),\n",
    "    \n",
    "    (\"What is feature engineering? How does it affect the model’s performance?\", \n",
    "     \"Feature engineering refers to developing some new features by using existing features. Sometimes there is a very subtle mathematical relation between some features which if explored properly then the new features can be developed using those mathematical operations. Also, there are times when multiple pieces of information are clubbed and provided as a single data column. At those times developing new features and using them help us to gain deeper insights into the data as well as if the features derived are significant enough helps to improve the model’s performance a lot.\"),\n",
    "    \n",
    "    (\"What is a Hypothesis in Machine Learning?\", \n",
    "     \"A hypothesis is a term that is generally used in the Supervised machine learning domain. As we have independent features and target variables and we try to find an approximate function mapping from the feature space to the target variable that approximation of mapping is known as a hypothesis.\"),\n",
    "    \n",
    "    (\"How do measure the effectiveness of the clusters?\", \n",
    "     \"There are metrics like Inertia or Sum of Squared Errors (SSE), Silhouette Score, l1, and l2 scores. Out of all of these metrics, the Inertia or Sum of Squared Errors (SSE) and Silhouette score is a common metrics for measuring the effectiveness of the clusters. Although this method is quite expensive in terms of computation cost. The score is high if the clusters formed are dense and well separated.\"),\n",
    "    \n",
    "    (\"Why do we take smaller values of the learning rate?\", \n",
    "     \"Smaller values of learning rate help the training process to converge more slowly and gradually toward the global optimum instead of fluctuating around it. This is because a smaller learning rate results in smaller updates to the model weights at each iteration, which can help to ensure that the updates are more precise and stable. If the learning rate is too large, the model weights can update too quickly, which can cause the training process to overshoot the global optimum and miss it entirely. So, to avoid this oscillation of the error value and achieve the best weights for the model this is necessary to use smaller values of the learning rate.\"),\n",
    "    \n",
    "    (\"What is Overfitting in Machine Learning and how can it be avoided?\", \n",
    "     \"Overfitting happens when the model learns patterns as well as the noises present in the data this leads to high performance on the training data but very low performance for data that the model has not seen earlier. To avoid overfitting there are multiple methods that we can use: Early stopping of the model’s training in case of validation training stops increasing but the training keeps going on. Using regularization methods like L1 or L2 regularization which is used to penalize the model’s weights to avoid overfitting.\"),\n",
    "    \n",
    "    (\"Why we cannot use linear regression for a classification task?\", \n",
    "     \"The main reason why we cannot use linear regression for a classification task is that the output of linear regression is continuous and unbounded, while classification requires discrete and bounded output values. If we use linear regression for the classification task the error function graph will not be convex. A convex graph has only one minimum which is also known as the global minima but in the case of the non-convex graph, there are chances of our model getting stuck at some local minima which may not be the global minima. To avoid this situation of getting stuck at the local minima we do not use the linear regression algorithm for a classification task.\"),\n",
    "\n",
    "    (\"Why do we perform normalization?\", \n",
    "     \"To achieve stable and fast training of the model we use normalization techniques to bring all the features to a certain scale or range of values. If we do not perform normalization then there are chances that the gradient will not converge to the global or local minima and end up oscillating back and forth.\"),\n",
    "\n",
    "    (\"What is the difference between precision and recall?\", \n",
    "     \"Precision is simply the ratio between the true positives(TP) and all the positive examples (TP+FP) predicted by the model. In other words, precision measures how many of the predicted positive examples are actually true positives. It is a measure of the model’s ability to avoid false positives and make accurate positive predictions.But in the case of a recall, we calculate the ratio of true positives (TP) and the total number of examples (TP+FN) that actually fall in the positive class. recall measures how many of the actual positive examples are correctly identified by the model. It is a measure of the model’s ability to avoid false negatives and identify all positive examples correctly.\"),\n",
    "\n",
    "    (\"What is the difference between upsampling and downsampling?\", \n",
    "     \"In the upsampling method, we increase the number of samples in the minority class by randomly selecting some points from the minority class and adding them to the dataset repeat this process till the dataset gets balanced for each class. But here is a disadvantage the training accuracy becomes high as in each epoch model trained more than once in each epoch but the same high accuracy is not observed in the validation accuracy. In the case of downsampling, we decrease the number of samples in the majority class by selecting some random number of points that are equal to the number of data points in the minority class so that the distribution becomes balanced. In this case, we have to suffer from data loss which may lead to the loss of some critical information as well. \"),\n",
    "\n",
    "    (\"What is data leakage and how can we identify it?\", \n",
    "     \"If there is a high correlation between the target variable and the input features then this situation is referred to as data leakage. This is because when we train our model with that highly correlated feature then the model gets most of the target variable’s information in the training process only and it has to do very little to achieve high accuracy. In this situation, the model gives pretty decent performance both on the training as well as the validation data but as we use that model to make actual predictions then the model’s performance is not up to the mark. This is how we can identify data leakage.\"),\n",
    "\n",
    "    (\"Explain the classification report and the metrics it includes.\", \n",
    "     \"Classification reports are evaluated using classification metrics that have precision, recall, and f1-score on a per-class basis.Precision can be defined as the ability of a classifier not to label an instance positive that is actually negative. Recall is the ability of a classifier to find all positive values. For each class, it is defined as the ratio of true positives to the sum of true positives and false negatives. F1-score is a harmonic mean of precision and recall. Support is the number of samples used for each class.The overall accuracy score of the model is also there to get a high-level review of the performance. It is the ratio between the total number of correct predictions and the total number of datasets.Macro avg is nothing but the average of the metric(precision, recall, f1-score) values for each class. The weighted average is calculated by providing a higher preference to that class that was present in the higher number in the datasets.\"),\n",
    "\n",
    "    (\"What are some of the hyperparameters of the random forest regressor which help to avoid overfitting?\", \n",
    "     \"The most important hyper-parameters of a Random Forest are: max_depth - Sometimes the larger depth of the tree can create overfitting. To overcome it, the depth should be limited.n-estimator - It is the number of decision trees we want in our forest.min_sample_split - It is the minimum number of samples an internal node must hold in order to split into further nodes.max_leaf_nodes - It helps the model to control the splitting of the nodes and in turn, the depth of the model is also restricted.\"),\n",
    "    \n",
    "    (\"What is the bias-variance tradeoff?\", \n",
    "     \"First, let’s understand what is bias and variance:Bias refers to the difference between the actual values and the predicted values by the model. Low bias means the model has learned the pattern in the data and high bias means the model is unable to learn the patterns present in the data i.e the underfitting.Variance refers to the change in accuracy of the model’s prediction on which the model has not been trained. Low variance is a good case but high variance means that the performance of the training data and the validation data vary a lot.If the bias is too low but the variance is too high then that case is known as overfitting. So, finding a balance between these two situations is known as the bias-variance trade-off.\"),\n",
    "\n",
    "    (\"Is it always necessary to use an 80:20 ratio for the train test split?\", \n",
    "     \"No there is no such necessary condition that the data must be split into 80:20 ratio. The main purpose of the splitting is to have some data which the model has not seen previously so, that we can evaluate the performance of the model. If the dataset contains let’s say 50,000 rows of data then only 1000 or maybe 2000 rows of data is enough to evaluate the model’s performance.\"),\n",
    "\n",
    "    (\"What is Principal Component Analysis?\", \n",
    "     \"PCA(Principal Component Analysis) is an unsupervised machine learning dimensionality reduction technique in which we trade off some information or patterns of the data at the cost of reducing its size significantly. In this algorithm, we try to preserve the variance of the original dataset up to a great extent let’s say 95%. For very high dimensional data sometimes even at the loss of 1% of the variance, we can reduce the data size significantly. By using this algorithm we can perform image compression, visualize high-dimensional data as well as make data visualization easy.\"),\n",
    "\n",
    "    (\"What is one-shot learning?\", \n",
    "     \"One-shot learning is a concept in machine learning where the model is trained to recognize the patterns in datasets from a single example instead of training on large datasets. This is useful when we haven’t large datasets. It is applied to find the similarity and dissimilarities between the two images.\"),\n",
    "\n",
    "    (\"What is the difference between Manhattan Distance and Euclidean distance?\", \n",
    "     \"Both Manhattan Distance and Euclidean distance are two distance measurement techniques. Manhattan Distance (MD) is calculated as the sum of absolute differences between the coordinates of two points along each dimension. Euclidean Distance (ED) is calculated as the square root of the sum of squared differences between the coordinates of two points along each dimension.Generally, these two metrics are used to evaluate the effectiveness of the clusters formed by a clustering algorithm.\"),\n",
    "\n",
    "    (\"What is the difference between covariance and correlation?\", \n",
    "     \"As the name suggests, Covariance provides us with a measure of the extent to which two variables differ from each other. But on the other hand, correlation gives us the measure of the extent to which the two variables are related to each other. Covariance can take on any value while correlation is always between -1 and 1. These measures are used during the exploratory data analysis to gain insights from the data.\"),\n",
    "\n",
    "    (\"What is the difference between one hot encoding and ordinal encoding?\", \n",
    "     \"One Hot encoding and ordinal encoding both are different methods to convert categorical features to numeric ones the difference is in the way they are implemented. In one hot encoding, we create a separate column for each category and add 0 or 1 as per the value corresponding to that row. Contrary to one hot encoding, In ordinal encoding, we replace the categories with numbers from 0 to n-1 based on the order or rank where n is the number of unique categories present in the dataset. The main difference between one-hot encoding and ordinal encoding is that one-hot encoding results in a binary matrix representation of the data in the form of 0 and 1, it is used when there is no order or ranking between the dataset whereas ordinal encoding represents categories as ordinal values.\"),\n",
    "\n",
    "    (\"How to identify whether the model has overfitted the training data or not?\", \n",
    "     \"This is the step where the splitting of the data into training and validation data proves to be a boon. If the model’s performance on the training data is very high as compared to the performance on the validation data then we can say that the model has overfitted the training data by learning the patterns as well as the noise present in the dataset.\"),\n",
    "\n",
    "    (\"How can you conclude about the model’s performance using the confusion matrix?\", \n",
    "     \"confusion matrix summarizes the performance of a classification model. In a confusion matrix, we get four types of output (in case of a binary classification problem) which are TP, TN, FP, and FN. As we know that there are two diagonals possible in a square, and one of these two diagonals represents the numbers for which our model’s prediction and the true labels are the same. Our target is also to maximize the values along these diagonals. From the confusion matrix, we can calculate various evaluation metrics like accuracy, precision, recall, F1 score, etc.\"),\n",
    "    \n",
    "    (\"What is the use of the violin plot?\", \n",
    "     \"The name violin plot has been derived from the shape of the graph which matches the violin. This graph is an extension of the Kernel Density Plot along with the properties of the boxplot. All the statistical measures shown by a boxplot are also shown by the violin plot but along with this, The width of the violin represents the density of the variable in the different regions of values. This visualization tool is generally used in the exploratory data analysis step to check the distribution of the continuous data variables. With this, we have covered some of the most important Machine Learning concepts which are generally asked by the interviewers to test the technical understanding of a candidate also, we would like to wish you all the best for your next interview. \"),\n",
    "\n",
    "    (\"What is the difference between stochastic gradient descent (SGD) and gradient descent (GD)?\", \n",
    "     \"In the gradient descent algorithm train our model on the whole dataset at once. But in Stochastic Gradient Descent, the model is trained by using a mini-batch of training data at once. If we are using SGD then one cannot expect the training error to go down smoothly. The training error oscillates but after some training steps, we can say that the training error has gone down. Also, the minima achieved by using GD may vary from that achieved using the SGD. It is observed that the minima achieved by using SGD are close to GD but not the same. \"),\n",
    "\n",
    "    (\"What is the Central Limit theorem?\", \n",
    "     \"This theorem is related to sampling statistics and its distribution. As per this theorem the sampling distribution of the sample means tends to towards a normal distribution as the sample size increases. No matter how the population distribution is shaped. i.e if we take some sample points from the distribution and calculate its mean then the distribution of those mean points will follow a normal/gaussian distribution no matter from which distribution we have taken the sample points. There is one condition that the size of the sample must be greater than or equal to 30 for the CLT to hold. and the mean of the sample means approaches the population mean.\"),\n",
    "\n",
    "    (\"What is the difference between the k-means and k-means++ algorithms?\", \n",
    "     \"The only difference between the two is in the way centroids are initialized. In the k-means algorithm, the centroids are initialized randomly from the given points. There is a drawback in this method that sometimes this random initialization leads to non-optimized clusters due to maybe initialization of two clusters close to each other. To overcome this problem k-means++ algorithm was formed. In k-means++, The first centroid is selected randomly from the data points. The selection of subsequent centroids is based on their separation from the initial centroids. The probability of a point being selected as the next centroid is proportional to the squared distance between the point and the closest centroid that has already been selected. This guarantees that the centroids are evenly spread apart and lowers the possibility of convergence to less-than-ideal clusters. This helps the algorithm reach the global minima instead of getting stuck at some local minima. \"),\n",
    "\n",
    "    (\"What happens to the mean, median, and mode when your data distribution is right skewed and left skewed?\", \n",
    "     \"In the case of a left-skewed distribution also known as a positively skewed distribution mean is greater than the median which is greater than the mode. But in the case of left-skewed distribution, the scenario is completely reversed.\"),\n",
    "\n",
    "    (\"What is the difference between L1 and L2 regularization? What is their significance?\", \n",
    "     \"L1 regularization: In L1 regularization also known as Lasso regularization in which we add the sum of absolute values of the weights of the model in the loss function.In L1 regularization weights for those features which are not at all important are penalized to zero so, in turn, we obtain feature selection by using the L1 regularization technique.L2 regularization: In L2 regularization also known as Ridge regularization in which we add the square of the weights to the loss function. In both of these regularization methods, weights are penalized but there is a subtle difference between the objective they help to achieve. In L2 regularization the weights are not penalized to 0 but they are near zero for irrelevant features. It is often used to prevent overfitting by shrinking the weights towards zero, especially when there are many features and the data is noisy.\"),\n",
    "\n",
    "    (\"What is a radial basis function?\", \n",
    "     \"RBF (radial basis function) is a real-valued function used in machine learning whose value only depends upon the input and fixed point called the center. \"),\n",
    "\n",
    "    (\"Does the accuracy score always a good metric to measure the performance of a classification model?\", \n",
    "     \"No, there are times when we train our model on an imbalanced dataset the accuracy score is not a good metric to measure the performance of the model. In such cases, we use precision and recall to measure the performance of a classification model. Also, f1-score is another metric that can be used to measure performance but in the end, f1-score is also calculated using precision and recall as the f1-score is nothing but the harmonic mean of the precision and recall. \"),\n",
    "\n",
    "    (\"What is KNN Imputer?\", \n",
    "     \"We generally impute null values by the descriptive statistical measures of the data like mean, mode, or median but KNN Imputer is a more sophisticated method to fill the null values. A distance parameter is also used in this method which is also known as the k parameter. The work is somehow similar to the clustering algorithm. The missing value is imputed in reference to the neighborhood points of the missing values.\"),\n",
    "    \n",
    "    (\"What is the purpose of splitting a given dataset into training and validation data?\", \n",
    "     \"The main purpose is to keep some data left over on which the model has not been trained so, that we can evaluate the performance of our machine learning model after training. Also, sometimes we use the validation dataset to choose among the multiple state-of-the-art machine learning models. Like we first train some models let’s say LogisticRegression, XGBoost, or any other than test their performance using validation data and choose the model which has less difference between the validation and the training accuracy.\"),\n",
    "\n",
    "    (\"What is the difference between k-means and the KNN algorithm?\", \n",
    "     \"k-means algorithm is one of the popular unsupervised machine learning algorithms which is used for clustering purposes. But the KNN is a model which is generally used for the classification task and is a supervised machine learning algorithm. The k-means algorithm helps us to label the data by forming clusters within the dataset.\"),\n",
    "\n",
    "    (\"What is Linear Discriminant Analysis?\", \n",
    "     \"LDA is a supervised machine learning dimensionality reduction technique because it uses target variables also for dimensionality reduction. It is commonly used for classification problems. The LDA mainly works on two objectives:Maximize the distance between the means of the two classes.Minimize the variation within each class.\"),\n",
    "\n",
    "    (\"How can we visualize high-dimensional data in 2-d?\", \n",
    "     \"One of the most common and effective methods is by using the t-SNE algorithm which is a short form for t-Distributed Stochastic Neighbor Embedding. This algorithm uses some non-linear complex methods to reduce the dimensionality of the given data. We can also use PCA or LDA to convert n-dimensional data to 2 – dimensional so, that we can plot it to get visuals for better analysis. But the difference between the PCA and t-SNE is that the former tries to preserve the variance of the dataset but the t-SNE tries to preserve the local similarities in the dataset.\"),\n",
    "\n",
    "    (\"What is the reason behind the curse of dimensionality?\", \n",
    "     \"As the dimensionality of the input data increases the amount of data required to generalize or learn the patterns present in the data increases. For the model, it becomes difficult to identify the pattern for every feature from the limited number of datasets or we can say that the weights are not optimized properly due to the high dimensionality of the data and the limited number of examples used to train the model. Due to this after a certain threshold for the dimensionality of the input data, we have to face the curse of dimensionality.\"),\n",
    "\n",
    "    (\"Why removing highly correlated features are considered a good practice?\", \n",
    "     \"When two features are highly correlated, they may provide similar information to the model, which may cause overfitting. If there are highly correlated features in the dataset then they unnecessarily increase the dimensionality of the feature space and sometimes create the problem of the curse of dimensionality. If the dimensionality of the feature space is high then the model training may take more time than expected, it will increase the complexity of the model and chances of error. This somehow also helps us to achieve data compression as the features have been removed without much loss of data.\"),\n",
    "\n",
    "    (\"What is the difference between the content-based and collaborative filtering algorithms of recommendation systems?\", \n",
    "     \"n a content-based recommendation system, similarities in the content and services are evaluated, and then by using these similarity measures from past data we recommend products to the user. But on the other hand in collaborative filtering, we recommend content and services based on the preferences of similar users. For example, if one user has taken A and B services in past and a new user has taken service A then service A will be recommended to him based on the other user’s preferences.\"),\n",
    "\n",
    "    (\"How do you select important variables while working on a data set? \", \n",
    "     \"There are various means to select important variables from a data set that include the following:Identify and discard correlated variables before finalizing on important variables. The variables could be selected based on ‘p’ values from Linear Regression, Forward, Backward, and Stepwise selection. Lasso Regression. Random Forest and plot variable chart. Top features can be selected based on information gain for the available set of features.\"),\n",
    "\n",
    "    (\"State the differences between causality and correlation?\", \n",
    "     \"Causality applies to situations where one action, say X, causes an outcome, say Y, whereas Correlation is just relating one action (X) to another action(Y) but X does not necessarily cause Y.\"),\n",
    "    \n",
    "    (\"How do we apply Machine Learning to Hardware?\", \n",
    "     \"We have to build ML algorithms in System Verilog which is a Hardware development Language and then program it onto an FPGA to apply Machine Learning to hardware.\"),\n",
    "\n",
    "    (\"How can we relate standard deviation and variance?\", \n",
    "     \"Standard deviation refers to the spread of your data from the mean. Variance is the average degree to which each point differs from the mean i.e. the average of all data points. We can relate Standard deviation and Variance because it is the square root of Variance.\"),\n",
    "\n",
    "    (\"Is a high variance in data good or bad?\", \n",
    "     \"Higher variance directly means that the data spread is big and the feature has a variety of data. Usually, high variance in a feature is seen as not so good quality.\"),\n",
    "\n",
    "    (\"If your dataset is suffering from high variance, how would you handle it?\", \n",
    "     \"For datasets with high variance, we could use the bagging algorithm to handle it. Bagging algorithm splits the data into subgroups with sampling replicated from random data. After the data is split, random data is used to create rules using a training algorithm. Then we use polling technique to combine all the predicted outcomes of the model.\"),\n",
    "\n",
    "    (\"What is Time series?\", \n",
    "     \"A Time series is a sequence of numerical data points in successive order. It tracks the movement of the chosen data points, over a specified period of time and records the data points at regular intervals. Time series doesn’t require any minimum or maximum time input. Analysts often use Time series to examine data according to their specific requirement.\"),\n",
    "    \n",
    "    (\"What is a Box-Cox transformation?\", \n",
    "     \"Box-Cox transformation is a power transform which transforms non-normal dependent variables into normal variables as normality is the most common assumption made while using many statistical techniques. It has a lambda parameter which when set to 0 implies that this transform is equivalent to log-transform. It is used for variance stabilization and also to normalize the distribution.\"),\n",
    "\n",
    "    (\"What is the exploding gradient problem while using the back propagation technique?\", \n",
    "     \"When large error gradients accumulate and result in large changes in the neural network weights during training, it is called the exploding gradient problem. The values of weights can become so large as to overflow and result in NaN values. This makes the model unstable and the learning of the model to stall just like the vanishing gradient problem. This is one of the most commonly asked interview questions on machine learning.\"),\n",
    "\n",
    "    (\"Can you mention some advantages and disadvantages of decision trees?\", \n",
    "     \"The advantages of decision trees are that they are easier to interpret, are nonparametric and hence robust to outliers, and have relatively few parameters to tune.On the other hand, the disadvantage is that they are prone to overfitting.\"),\n",
    "\n",
    "    (\"What’s a Fourier transform?\", \n",
    "     \"Fourier Transform is a mathematical technique that transforms any function of time to a function of frequency. Fourier transform is closely related to Fourier series. It takes any time-based pattern for input and calculates the overall cycle offset, rotation speed and strength for all possible cycles. Fourier transform is best applied to waveforms since it has functions of time and space. Once a Fourier transform applied on a waveform, it gets decomposed into a sinusoid.\"),\n",
    "\n",
    "    (\"What do you mean by Associative Rule Mining (ARM)?\", \n",
    "     \"Associative Rule Mining is one of the techniques to discover patterns in data like features (dimensions) which occur together and features (dimensions) which are correlated. It is mostly used in Market-based Analysis to find how frequently an itemset occurs in a transaction. Association rules have to satisfy minimum support and minimum confidence at the very same time. Association rule generation generally comprised of two different steps:“A min support threshold is given to obtain all frequent item-sets in a database.”“A min confidence constraint is given to these frequent item-sets in order to form the association rules.”Support is a measure of how often the “item set” appears in the data set and Confidence is a measure of how often a particular rule has been found to be true.\"),\n",
    "    \n",
    "    (\"What is Marginalisation?\", \n",
    "     \"Marginalisation is summing the probability of a random variable X given joint probability distribution of X with other variables. It is an application of the law of total probability.\"),\n",
    "\n",
    "    (\"When does the linear regression line stop rotating or finds an optimal spot where it is fitted on data? \", \n",
    "     \"A place where the highest RSquared value is found, is the place where the line comes to rest. RSquared represents the amount of variance captured by the virtual linear regression line with respect to the total variance captured by the dataset. \"),\n",
    "\n",
    "    (\"Why is logistic regression a type of classification technique and not a regression? Name the function it is derived from? \", \n",
    "     \"Since the target column is categorical, it uses linear regression to create an odd function that is wrapped with a log function to use regression as a classifier. Hence, it is a type of classification technique and not a regression. It is derived from cost function. \"),\n",
    "\n",
    "    (\"What could be the issue when the beta value for a certain variable varies way too much in each subset when regression is run on different subsets of the given dataset?\", \n",
    "     \"Variations in the beta values in every subset implies that the dataset is heterogeneous. To overcome this problem, we can use a different model for each of the dataset’s clustered subsets or a non-parametric model such as decision trees.\"),\n",
    "\n",
    "    (\"What does the term Variance Inflation Factor mean?\", \n",
    "     \"Variation Inflation Factor (VIF) is the ratio of the model’s variance to the model’s variance with only one independent variable. VIF gives the estimate of the volume of multicollinearity in a set of many regression variables.VIF = Variance of the model with one independent variable\"),\n",
    "    \n",
    "    (\"Which machine learning algorithm is known as the lazy learner, and why is it called so?\", \n",
    "     \"KNN is a Machine Learning algorithm known as a lazy learner. K-NN is a lazy learner because it doesn’t learn any machine-learned values or variables from the training data but dynamically calculates distance every time it wants to classify, hence memorizing the training dataset instead. \"),\n",
    "\n",
    "    (\"Is it possible to use KNN for image processing? \", \n",
    "     \"Yes, it is possible to use KNN for image processing. It can be done by converting the 3-dimensional image into a single-dimensional vector and using the same as input to KNN. \"),\n",
    "\n",
    "    (\"How does the SVM algorithm deal with self-learning? \", \n",
    "     \"SVM has a learning rate and expansion rate which takes care of this. The learning rate compensates or penalises the hyperplanes for making all the wrong moves and expansion rate deals with finding the maximum separation area between classes.\"),\n",
    "\n",
    "    (\"What are Kernels in SVM?\", \n",
    "     \"The function of the kernel is to take data as input and transform it into the required form. A few popular Kernels used in SVM are as follows: RBF, Linear, Sigmoid, Polynomial, Hyperbolic, Laplace, etc. \"),\n",
    "\n",
    "    (\"What are ensemble models?\", \n",
    "     \"An ensemble is a group of models that are used together for prediction both in classification and regression classes. Ensemble learning helps improve ML results because it combines several models. By doing so, it allows for a better predictive performance compared to a single model. \"),\n",
    "    \n",
    "    (\"What is OOB error and how does it occur? \", \n",
    "     \"For each bootstrap sample, there is one-third of the data that was not used in the creation of the tree, i.e., it was out of the sample. This data is referred to as out of bag data. In order to get an unbiased measure of the accuracy of the model over test data, out of bag error is used. The out of bag data is passed for each tree is passed through that tree and the outputs are aggregated to give out of bag error. This percentage error is quite effective in estimating the error in the testing set and does not require further cross-validation. \"),\n",
    "\n",
    "    (\"Why boosting is a more stable algorithm as compared to other ensemble algorithms? \", \n",
    "     \"Boosting focuses on errors found in previous iterations until they become obsolete. Whereas in bagging there is no corrective loop. This is why boosting is a more stable algorithm compared to other ensemble algorithms. \"),\n",
    "\n",
    "    (\"What is Naive Bayes?\", \n",
    "     \"Naive Bayes classifiers are a series of classification algorithms that are based on the Bayes theorem. This family of algorithm shares a common principle which treats every pair of features independently while being classified. \"),\n",
    "\n",
    "    (\"Why Naive Bayes is called Naive?\", \n",
    "     \"Naive Bayes is considered Naive because the attributes in it (for the class) is independent of others in the same class.  This lack of dependence between two attributes of the same class creates the quality of naiveness.\"),\n",
    "\n",
    "    (\"What’s the difference between probability and likelihood?\", \n",
    "     \"Probability is the measure of the likelihood that an event will occur that is, what is the certainty that a specific event will occur? Where-as a likelihood function is a function of parameters within the parameter space that describes the probability of obtaining the observed data. So the fundamental difference is, Probability attaches to possible results; likelihood attaches to hypotheses. \"),\n",
    "    \n",
    "    (\"What is the difference between a generative and discriminative model?\", \n",
    "     \"A generative model learns the different categories of data. On the other hand, a discriminative model will only learn the distinctions between different categories of data. Discriminative models perform much better than the generative models when it comes to classification tasks.\"),\n",
    "\n",
    "    (\"What are hyperparameters and how are they different from parameters?\", \n",
    "     \"A parameter is a variable that is internal to the model and whose value is estimated from the training data. They are often saved as part of the learned model. Examples include weights, biases etc.A hyperparameter is a variable that is external to the model whose value cannot be estimated from the data. They are often used to estimate model parameters. The choice of parameters is sensitive to implementation. Examples include learning rate, hidden layers etc.\"),\n",
    "\n",
    "    (\"What is the default method of splitting in decision trees?\", \n",
    "     \"The default method of splitting in decision trees is the Gini Index. Gini Index is the measure of impurity of a particular node.This can be changed by making changes to classifier parameters. \"),\n",
    "\n",
    "    (\"How is p-value useful?\", \n",
    "     \"The p-value gives the probability of the null hypothesis is true. It gives us the statistical significance of our results. In other words, p-value determines the confidence of a model in a particular output.\"),\n",
    "\n",
    "    (\"What are the hyperparameters of a logistic regression model?\", \n",
    "     \"Classifier penalty, classifier solver and classifier C are the trainable hyperparameters of a Logistic Regression Classifier. These can be specified exclusively with values in Grid Search to hyper tune a Logistic Classifier.\"),\n",
    "    \n",
    "    (\"How to deal with multicollinearity?\", \n",
    "     \"Multi collinearity can be dealt with by the following steps:Remove highly correlated predictors from the model.Use Partial Least Squares Regression (PLS) or Principal Components Analysis\"),\n",
    "\n",
    "    (\"What is Heteroscedasticity?\", \n",
    "     \"t is a situation in which the variance of a variable is unequal across the range of values of the predictor variable.It should be avoided in regression as it introduces unnecessary variance.  \"),\n",
    "\n",
    "    (\"What is the role of cross-validation?\", \n",
    "     \"Cross-validation is a technique which is used to increase the performance of a machine learning algorithm, where the machine is fed sampled data out of the same data for a few times. The sampling is done so that the dataset is broken into small parts of the equal number of rows, and a random part is chosen as the test set, while all other parts are chosen as train sets.\"),\n",
    "\n",
    "    (\"What is a voting model?\", \n",
    "     \"A voting model is an ensemble model which combines several classifiers but to produce the final result, in case of a classification-based model, takes into account, the classification of a certain data point of all the models and picks the most vouched/voted/generated option from all the given classes in the target column.\"),\n",
    "\n",
    "    (\"How to deal with very few data samples? Is it possible to make a model out of it?\", \n",
    "     \"If very few data samples are there, we can make use of oversampling to produce new data points. In this way, we can have new data points.\"),\n",
    "    \n",
    "    (\"What impact does correlation have on PCA?\", \n",
    "     \"If data is correlated PCA does not work well. Because of the correlation of variables the effective variance of variables decreases. Hence correlated data when used for PCA does not work well.\"),\n",
    "\n",
    "    (\"Which metrics can be used to measure correlation of categorical data?\", \n",
    "     \"Chi square test can be used for doing so. It gives the measure of correlation between categorical predictors.\"),\n",
    "\n",
    "    (\"Which algorithm can be used in value imputation in both categorical and continuous categories of data?\", \n",
    "     \"KNN is the only algorithm that can be used for imputation of both categorical and continuous variables.\"),\n",
    "\n",
    "    (\"When should ridge regression be preferred over lasso?\", \n",
    "     \"We should use ridge regression when we want to use all predictors and not remove any as it reduces the coefficient values but does not nullify them.\"),\n",
    "\n",
    "    (\"Which algorithms can be used for important variable selection?\", \n",
    "     \"Random Forest, Xgboost and plot variable importance charts can be used for variable selection.\"),\n",
    "    \n",
    "    (\"What ensemble technique is used by Random forests?\", \n",
    "     \"Bagging is the technique used by Random Forests. Random forests are a collection of trees which work on sampled data from the original dataset with the final prediction being a voted average of all trees.\"),\n",
    "\n",
    "    (\"If we have a high bias error what does it mean? \", \n",
    "     \"High bias error means that that model we are using is ignoring all the important trends in the model and the model is underfitting.\"),\n",
    "\n",
    "    (\"Which type of sampling is better for a classification model and why?\", \n",
    "     \"Stratified sampling is better in case of classification problems because it takes into account the balance of classes in train and test sets. The proportion of classes is maintained and hence the model performs better. In case of random sampling of data, the data is divided into two parts without taking into consideration the balance classes in the train and test sets. Hence some classes might be present only in tarin sets or validation sets. Hence the results of the resulting model are poor in this case.\"),\n",
    "\n",
    "    (\"When can be a categorical value treated as a continuous variable and what effect does it have when done so?\", \n",
    "     \"A categorical predictor can be treated as a continuous one when the nature of data points it represents is ordinal. If the predictor variable is having ordinal data then it can be treated as continuous and its inclusion in the model increases the performance of the model.\"),\n",
    "\n",
    "    (\"What is a pipeline?\", \n",
    "     \"A pipeline is a sophisticated way of writing software such that each intended action while building a model can be serialized and the process calls the individual functions for the individual tasks. The tasks are carried out in sequence for a given sequence of data points and the entire process can be run onto n threads by use of composite estimators in scikit learn.\"),\n",
    "    \n",
    "    (\"Which sampling technique is most suitable when working with time-series data?\", \n",
    "     \"We can use a custom iterative sampling such that we continuously add samples to the train set. We only should keep in mind that the sample used for validation should be added to the next train sets and a new sample is used for validation.\"),\n",
    "\n",
    "    (\"What is normal distribution?\", \n",
    "     \"The distribution having the below properties is called normal distribution. The mean, mode and median are all equal.The curve is symmetric at the center (i.e. around the mean, μ).Exactly half of the values are to the left of center and exactly half the values are to the right.The total area under the curve is 1.\"),\n",
    "\n",
    "    (\"What is the 68 per cent rule in normal distribution?\", \n",
    "     \"The normal distribution is a bell-shaped curve. Most of the data points are around the median. Hence approximately 68 per cent of the data is around the median. Since there is no skewness and its bell-shaped. \"),\n",
    "\n",
    "    (\"What is a random variable?\", \n",
    "     \"A Random Variable is a set of possible values from a random experiment.\"),\n",
    "\n",
    "    (\"What is the degree of freedom?\", \n",
    "     \"It is the number of independent values or quantities which can be assigned to a statistical distribution. It is used in Hypothesis testing and chi-square test.\"),\n",
    "    \n",
    "    (\"What is a false positive?\", \n",
    "     \"It is a test result which wrongly indicates that a particular condition or attribute is present.\"),\n",
    "\n",
    "    (\"What is a false negative?\", \n",
    "     \"A test result which wrongly indicates that a particular condition or attribute is absent.\"),\n",
    "\n",
    "    (\"What is the error term composed of in regression?\", \n",
    "     \"Error is a sum of bias error+variance error+ irreducible error in regression. Bias and variance error can be reduced but not the irreducible error.\"),\n",
    "\n",
    "    (\"Which performance metric is better R2 or adjusted R2?\", \n",
    "     \"Type I and Type II error in machine learning refers to false values. Type I is equivalent to a False positive while Type II is equivalent to a False negative. In Type I error, a hypothesis which ought to be accepted doesn’t get accepted. Similarly, for Type II error, the hypothesis gets rejected which should have been accepted in the first place.\"),\n",
    "\n",
    "    (\"What do you mean by AUC curve?\", \n",
    "     \"AUC (area under curve). Higher the area under the curve, better the prediction power of the model.\"),\n",
    "    \n",
    "    (\"Why does XGBoost perform better than SVM?\", \n",
    "     \"First reason is that XGBoos is an ensemble method that uses many trees to make a decision so it gains power by repeating itself.SVM is a linear separator, when data is not linearly separable SVM needs a Kernel to project the data into a space where it can separate it, there lies its greatest strength and weakness, by being able to project data into a high dimensional space SVM can find a linear separation for almost any data but at the same time it needs to use a Kernel and we can argue that there’s not a perfect kernel for every dataset.\"),\n",
    "\n",
    "    (\"How is linear classifier relevant to SVM?\", \n",
    "     \"An svm is a type of linear classifier. If you don’t mess with kernels, it’s arguably the most simple type of linear classifier.Linear classifiers (all?) learn linear fictions from your data that map your input to scores like so: scores = Wx + b. Where W is a matrix of learned weights, b is a learned bias vector that shifts your scores, and x is your input data. This type of function may look familiar to you if you remember y = mx + b from high school.A typical svm loss function ( the function that tells you how good your calculated scores are in relation to the correct labels ) would be hinge loss. It takes the form: Loss = sum over all scores except the correct score of max(0, scores – scores(correct class) + 1).\"),\n",
    "\n",
    "    (\"Are Gaussian Naive Bayes the same as binomial Naive Bayes?\", \n",
    "     \"Binomial Naive Bayes: It assumes that all our features are binary such that they take only two values. Means 0s can represent “word does not occur in the document” and 1s as “word occurs in the document”.Gaussian Naive Bayes: Because of the assumption of the normal distribution, Gaussian Naive Bayes is used in cases when all our features are continuous. For example in Iris dataset features are sepal width, petal width, sepal length, petal length. So its features can have different values in the data set as width and length can vary. We can’t represent features in terms of their occurrences. This means data is continuous. Hence we use Gaussian Naive Bayes here.\"),\n",
    "    \n",
    "    (\"What do you understand by Precision and Recall?\", \n",
    "     \"In pattern recognition, The information retrieval and classification in machine learning are part of precision. It is also called as positive predictive value which is the fraction of relevant instances among the retrieved instances.Recall is also known as sensitivity and the fraction of the total amount of relevant instances which  were actually retrieved. Both precision and recall are therefore based on an understanding and measure of relevance.\"),\n",
    "    \n",
    "    (\"What is the difference between Entropy and Information Gain?\", \n",
    "     \"The information gain is based on the decrease in entropy after a dataset is split on an attribute. Constructing a decision tree is all about finding the attribute that returns the highest information gain (i.e., the most homogeneous branches). Step 1: Calculate entropy of the target.\"),\n",
    "\n",
    "    (\"What’s the trade-off between bias and variance?\", \n",
    "     \"Bias is error due to erroneous or overly simplistic assumptions in the learning algorithm you’re using. This can lead to the model underfitting your data, making it hard for it to have high predictive accuracy and for you to generalize your knowledge from the training set to the test set.Variance is error due to too much complexity in the learning algorithm you’re using. This leads to the algorithm being highly sensitive to high degrees of variation in your training data, which can lead your model to overfit the data. You’ll be carrying too much noise from your training data for your model to be very useful for your test data.The bias-variance decomposition essentially decomposes the learning error from any algorithm by adding the bias, the variance and a bit of irreducible error due to noise in the underlying dataset. Essentially, if you make the model more complex and add more variables, you’ll lose bias but gain some variance — in order to get the optimally reduced amount of error, you’ll have to tradeoff bias and variance. You don’t want either high bias or high variance in your model.\"),\n",
    "\n",
    "    (\"How is KNN different from k-means clustering?\", \n",
    "     \"K-Nearest Neighbors is a supervised classification algorithm, while k-means clustering is an unsupervised clustering algorithm. While the mechanisms may seem similar at first, what this really means is that in order for K-Nearest Neighbors to work, you need labeled data you want to classify an unlabeled point into (thus the nearest neighbor part). K-means clustering requires only a set of unlabeled points and a threshold: the algorithm will take unlabeled points and gradually learn how to cluster them into groups by computing the mean of the distance between different points.The critical difference here is that KNN needs labeled points and is thus supervised learning, while k-means doesn’t—and is thus unsupervised learning.\"),\n",
    "\n",
    "    (\"Explain how a ROC curve works.\", \n",
    "     \"The ROC curve is a graphical representation of the contrast between true positive rates and the false positive rate at various thresholds. It’s often used as a proxy for the trade-off between the sensitivity of the model (true positives) vs the fall-out or the probability it will trigger a false alarm (false positives).\"),\n",
    "    \n",
    "    (\"What’s the difference between Type I and Type II error?\", \n",
    "     \"Type I error is a false positive, while Type II error is a false negative. Briefly stated, Type I error means claiming something has happened when it hasn’t, while Type II error means that you claim nothing is happening when in fact something is.\"),\n",
    "\n",
    "    (\"how does deep learning contrast with other machine learning algorithms?\", \n",
    "     \"Deep learning is a subset of machine learning that is concerned with neural networks: how to use backpropagation and certain principles from neuroscience to more accurately model large sets of unlabelled or semi-structured data. In that sense, deep learning represents an unsupervised learning algorithm that learns representations of data through the use of neural nets.\"),\n",
    "\n",
    "    (\"What’s the “kernel trick” and how is it useful?\", \n",
    "     \"he Kernel trick involves kernel functions that can enable in higher-dimension spaces without explicitly calculating the coordinates of points within that dimension: instead, kernel functions compute the inner products between the images of all pairs of data in a feature space. This allows them the very useful attribute of calculating the coordinates of higher dimensions while being computationally cheaper than the explicit calculation of said coordinates. Many algorithms can be expressed in terms of inner products. Using the kernel trick enables us effectively run algorithms in a high-dimensional space with lower-dimensional data.\"),\n",
    "\n",
    "    (\"How do you handle missing or corrupted data in a dataset?\", \n",
    "     \"You could find missing/corrupted data in a dataset and either drop those rows or columns, or decide to replace them with another value.In Pandas, there are two very useful methods: isnull() and dropna() that will help you find columns of data with missing or corrupted data and drop those values. If you want to fill the invalid values with a placeholder value (for example, 0), you could use the fillna() method.\")\n",
    "\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "dataf = pd.DataFrame(data, columns=['Question', 'Answer'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(dataf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interview queations on paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ML_Q_A.txt') as f:\n",
    "    lines = f.readlines()\n",
    "quest = []\n",
    "ans = []\n",
    "global_ans = []\n",
    "answer= False\n",
    "count = 0\n",
    "for line in lines:\n",
    "    line = line.replace('\\n', '')\n",
    "    if '&' in line and answer == False:\n",
    "        answer = True\n",
    "        count+=1\n",
    "    elif '&' in line and answer == True:\n",
    "        answer = False\n",
    "        count+=1\n",
    "\n",
    "    if line != '':\n",
    "        if answer:\n",
    "            ans.append(line.replace('&', ''))\n",
    "        else:\n",
    "            quest.append(line.replace('&', ''))\n",
    "    if count == 2:\n",
    "        count = 0\n",
    "        global_ans.append(''.join(ans))\n",
    "        ans = []\n",
    "global_ans.append(ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Question  \\\n",
      "0                    What is Artificial Intelligence?   \n",
      "1                 What are the different types of AI?   \n",
      "2           Explain the concept of Machine Learning.?   \n",
      "3    What is the difference between supervised and...   \n",
      "4         Define Deep Learning and its applications.?   \n",
      "..                                                ...   \n",
      "95   What are the challenges of AI in transportation?   \n",
      "96  Describe the concept of sentiment analysis in ...   \n",
      "97  What is the difference between data preprocess...   \n",
      "98  Explain the concept of transfer learning in co...   \n",
      "99   How does the Hierarchical Clustering algorith...   \n",
      "\n",
      "                                               Answer  \n",
      "0   Artificial Intelligence (AI) refers to the fie...  \n",
      "1   There are typically four different types or ca...  \n",
      "2   Machine Learning (ML) is a subfield of Artific...  \n",
      "3   The main difference between supervised and uns...  \n",
      "4   Deep Learning is a subfield of Machine Learnin...  \n",
      "..                                                ...  \n",
      "95  AI has the potential to revolutionize the tran...  \n",
      "96  Sentiment analysis, also known as opinion mini...  \n",
      "97  Data preprocessing and data cleaning are two i...  \n",
      "98  Transfer learning is a technique used in compu...  \n",
      "99  [Hierarchical Clustering is an unsupervised ma...  \n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame(\n",
    "    {'Question': quest,\n",
    "     'Answer': global_ans\n",
    "    })\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read dataset with generated questions based on newest articles and merge them with previous datasets into one giant dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('lora_modified.tsv', sep='\\t')\n",
    "joined = pd.concat([df, dataf, data,df1], ignore_index=True)\n",
    "joined.to_csv('final_data.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
